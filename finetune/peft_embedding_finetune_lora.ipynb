{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c15f65b-0b39-4057-8938-0dd834cf4522",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tune a state-of-the-art embedding with LoRA and SageMaker training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9ba97-ce7c-4569-ad1f-ac52f108fd52",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420ad81-452d-4ecd-8a91-cd8b54a9620f",
   "metadata": {},
   "source": [
    "Following the evaluation part, we'd like now to fine-tune our embedding with Shopping Queries Dataset. We want this fine-tuning to be cost efficient; but we also want to avoid catastrophic forgetting. Hence, we'll finetune it with LoRA. Thanks to Hugging Face, just by adding 3 lines of code, we can save costs while maintaining performance improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f6307-e329-4795-b048-6ff4b8f5f50e",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233fd56-12a0-46d0-989b-790c9fd5d1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca8247e-c2ba-4694-ac6d-5cf786cc2ff9",
   "metadata": {},
   "source": [
    "## LoRA in a nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143edb7a-1061-44f9-a9c1-11763da86220",
   "metadata": {},
   "source": [
    "__TO DO__ : explain the principle / Refer to blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a086d-a774-4593-a1cd-25056dc18fc7",
   "metadata": {},
   "source": [
    "## Create training script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5be0ee-3944-4735-9169-0b79857fef9f",
   "metadata": {},
   "source": [
    "Let's create a folder with training script and requirements files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e333488e-8a46-4079-b4b9-b3b4a7980662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p peftscripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff2da0-c73e-4b1d-ba73-b3b4b0fe812f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile peftscripts/train.py\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "\n",
    "from peft import TaskType\n",
    "from peft import LoraConfig, get_peft_model \n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "import os\n",
    "\n",
    "\n",
    "MAX_LENGTH=70\n",
    "\n",
    "\n",
    "class EncoderForESCI(nn.Module):\n",
    "    def __init__(self, pretrained_model, lora = True, normalize=True, lora_rank=8):\n",
    "        super(EncoderForESCI, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.model = AutoModel.from_pretrained(pretrained_model)\n",
    "        if lora:\n",
    "            config = LoraConfig(\n",
    "                r=lora_rank,\n",
    "                lora_alpha=32,\n",
    "                #target_modules=[\"key\",\"query\", \"value\"],\n",
    "                #target_modules=[\"encoder.layer.*\"],\n",
    "                target_modules=None,\n",
    "                bias=\"none\",\n",
    "                lora_dropout=0.05,\n",
    "                inference_mode=False,\n",
    "                task_type=TaskType.FEATURE_EXTRACTION\n",
    "            )\n",
    "        \n",
    "            self.model = get_peft_model(self.model, config)\n",
    "        self.normalize=normalize\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        model_output = self.model(**kwargs)\n",
    "        \n",
    "        embeddings = self.mean_pooling(model_output, kwargs[\"attention_mask\"])\n",
    "        if self.normalize:\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings\n",
    "      \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable: {trainable_params} || all: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "def tokenize_examples_and_target(examples):\n",
    "    queries = examples[\"query\"]\n",
    "    result = finetuned.tokenizer(queries, padding=\"max_length\", max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "    result = {f\"query_{k}\": v.reshape(-1) for k, v in result.items()}\n",
    "\n",
    "    products = examples[\"product_title\"]\n",
    "    result_products = finetuned.tokenizer(products, padding=\"max_length\", max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "    for k, v in result_products.items():\n",
    "        result[f\"product_{k}\"] = v.reshape(-1)\n",
    "\n",
    "    result[\"labels\"] = torch.ByteTensor([examples[\"relevance_label\"]]).reshape(-1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def full_forward_pass(batch, finetuned):\n",
    "    q = {k.replace(\"query_\", \"\"):v.to('cuda') for k,v in batch.items() if 'query_' in k} \n",
    "    p = {k.replace(\"product_\", \"\"):v.to('cuda') for k,v in batch.items() if 'product_' in k} \n",
    "\n",
    "    q_emb, p_emb = finetuned(**q), finetuned(**p)\n",
    "    labels=batch['labels'].reshape(-1).to('cuda')\n",
    "    return q_emb, p_emb, labels\n",
    "\n",
    "emb_loss=nn.CosineEmbeddingLoss(reduction=\"mean\")\n",
    "def get_loss(q_e, p_e, labels):\n",
    "    #In cosine loss, targets are 1 or -1, so we transform our 0/1 labels\n",
    "    return emb_loss(q_e, p_e, 2*labels - 1)\n",
    "    #return torch.square(emb_loss(q_e, p_e, 2*labels - 1)) \n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--pretrained_model')\n",
    "    parser.add_argument('--batch_size', type=int)\n",
    "    parser.add_argument('--eval_batch_size', type=int)\n",
    "\n",
    "    parser.add_argument('--epochs', type=int)\n",
    "    parser.add_argument('--lora_rank', default=8, type=int)\n",
    "    parser.add_argument('--lr', default=1e-6, type=float)\n",
    "\n",
    "    \n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    \n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    \n",
    "    finetuned = EncoderForESCI(args.pretrained_model, lora_rank=args.lora_rank)\n",
    "    \n",
    "    ds= load_dataset(\"smangrul/amazon_esci\")\n",
    "    processed_ds = ds.map(tokenize_examples_and_target,\n",
    "                      num_proc=os.cpu_count(), \n",
    "                     remove_columns=ds['train'].column_names)\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        processed_ds['train'],\n",
    "        shuffle=True,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=True)\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        processed_ds['validation'],\n",
    "        shuffle=True,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=args.eval_batch_size,\n",
    "        pin_memory=True)\n",
    "    \n",
    "    starting_epoch=0\n",
    "    finetuned.to('cuda')\n",
    "    optimizer = torch.optim.AdamW(finetuned.parameters(), lr=args.lr)\n",
    "    for epoch in range(starting_epoch, args.epochs):\n",
    "    \n",
    "        finetuned.train()\n",
    "        running_loss = 0.0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):            \n",
    "\n",
    "            q_emb, p_emb, labels = full_forward_pass(batch, finetuned)        \n",
    "            loss = get_loss(q_emb, p_emb,labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.detach().item()\n",
    "            if step%100==0:\n",
    "                mean_train_loss= running_loss / (step+1)\n",
    "                print(f\"Epoch {epoch+1}, step {step+1} ==> training loss {mean_train_loss}\")\n",
    "\n",
    "\n",
    "        mean_train_loss= running_loss / (step+1)\n",
    "        print(f\"Epoch {epoch+1}, step {step+1} ==> training loss {mean_train_loss}\")\n",
    "\n",
    "        finetuned.eval()\n",
    "        eval_loss=0.0\n",
    "        for step, batch in enumerate(tqdm(validation_dataloader)):\n",
    "            with torch.no_grad():\n",
    "                q_emb, p_emb, labels = full_forward_pass(batch, finetuned)\n",
    "                loss = get_loss(q_emb, p_emb,labels)\n",
    "            eval_loss+=loss.detach().item()\n",
    "            if step%100==0:\n",
    "                mean_valid_loss= eval_loss / (step+1)\n",
    "                print(f\"Epoch {epoch+1}, step {step+1} ==> eval loss {mean_valid_loss}\")\n",
    "\n",
    "\n",
    "        mean_valid_loss= eval_loss / (step+1)\n",
    "        print(f\"Finished Epoch {epoch+1}, step {step+1} ==> eval loss {mean_valid_loss}\")\n",
    "    \n",
    "    ## Merge the models with trained Adapters, and save it to /opt/ml/code\n",
    "    merged = finetuned.model.merge_and_unload()\n",
    "    merged.save_pretrained(args.model_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bdee7f-e234-4f6a-9f75-4bf85e05ff6e",
   "metadata": {},
   "source": [
    "As you can see, it's not so different from your classical PyTorch script. Of course, hyperparameters can be upgraded: I didn't put any learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d77ff-f43e-4f2c-a7f4-cf11ee58e289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile peftscripts/requirements.txt\n",
    "\n",
    "peft\n",
    "accelerate==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafeec4-1502-46d0-a70f-4be93ee7738c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'pretrained_model': \"BAAI/bge-base-en\",\n",
    "                 'batch_size': 120,\n",
    "                 'eval_batch_size': 160,\n",
    "                 'lora_rank':8, \n",
    "                 'epochs':1, \n",
    "                 'lr':1e-6\n",
    "                 }\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./peftscripts',\n",
    "                            instance_type='ml.g4dn.xlarge',\n",
    "                            instance_count=1,\n",
    "                            role = sagemaker.get_execution_role(),\n",
    "                            transformers_version='4.26',\n",
    "                            pytorch_version='1.13',\n",
    "                            py_version='py39',\n",
    "                            hyperparameters = hyperparameters, \n",
    "                             metric_definitions=[\n",
    "                                 {'Name': 'training_loss', 'Regex': 'training loss ([0-9\\\\.]+)'},\n",
    "                                 {'Name': 'eval_loss', 'Regex': 'eval loss ([0-9\\\\.]+)'}\n",
    "                             ]\n",
    ")\n",
    "\n",
    "timing = str(int(time.time()))\n",
    "huggingface_estimator.fit(job_name=f\"PeftFTBGE{timing}\", wait=False)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
