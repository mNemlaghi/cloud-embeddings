{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c15f65b-0b39-4057-8938-0dd834cf4522",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tune a state-of-the-art embedding with LoRA and SageMaker training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9ba97-ce7c-4569-ad1f-ac52f108fd52",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420ad81-452d-4ecd-8a91-cd8b54a9620f",
   "metadata": {},
   "source": [
    "Following the evaluation part, we'd like now to fine-tune our embedding with Shopping Queries Dataset. We want this fine-tuning to be cost efficient; but we also want to avoid catastrophic forgetting. Hence, we'll finetune it with LoRA. Thanks to Hugging Face, just by adding 3 lines of code, we can save costs while maintaining performance improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f6307-e329-4795-b048-6ff4b8f5f50e",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233fd56-12a0-46d0-989b-790c9fd5d1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca8247e-c2ba-4694-ac6d-5cf786cc2ff9",
   "metadata": {},
   "source": [
    "## LoRA in a nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143edb7a-1061-44f9-a9c1-11763da86220",
   "metadata": {},
   "source": [
    "__TO DO__ : explain the principle / Refer to blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a086d-a774-4593-a1cd-25056dc18fc7",
   "metadata": {},
   "source": [
    "## Create training script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5be0ee-3944-4735-9169-0b79857fef9f",
   "metadata": {},
   "source": [
    "Let's create a folder with training script and requirements files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e333488e-8a46-4079-b4b9-b3b4a7980662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p peftscripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ff2da0-c73e-4b1d-ba73-b3b4b0fe812f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting peftscripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile peftscripts/train.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import TrainingArguments, Trainer, logging\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "\n",
    "from peft import TaskType\n",
    "from peft import LoraConfig, get_peft_model \n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "import os\n",
    "#import evaluate\n",
    "\n",
    "\n",
    "#roc_auc_score = evaluate.load(\"roc_auc\")\n",
    "\n",
    "MAX_LENGTH=240\n",
    "\n",
    "\n",
    "class EncoderForESCI(nn.Module):\n",
    "    def __init__(self, pretrained_model, lora = True, normalize=True, lora_rank=8):\n",
    "        super(EncoderForESCI, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        self.embedding_model = AutoModel.from_pretrained(pretrained_model)\n",
    "        if lora:\n",
    "            config = LoraConfig(\n",
    "                r=lora_rank,\n",
    "                lora_alpha=16,\n",
    "                target_modules=[\"key\",\"query\", \"value\"],\n",
    "                #target_modules=[\"encoder.layer.*\"],\n",
    "                bias=\"none\",\n",
    "                lora_dropout=0.05,\n",
    "                inference_mode=False,\n",
    "                task_type=TaskType.FEATURE_EXTRACTION\n",
    "            )\n",
    "        \n",
    "            self.embedding_model = get_peft_model(self.embedding_model, config)\n",
    "        self.normalize=normalize\n",
    "        \n",
    "    def forward(self, query_input_ids, query_attention_mask, product_input_ids, product_attention_mask,\n",
    "                query_token_type_ids, product_token_type_ids, labels):\n",
    "        q = {\"input_ids\":query_input_ids.squeeze(1), \"attention_mask\":query_attention_mask.squeeze(1),\n",
    "            \"token_type_ids\":query_token_type_ids.squeeze(1)}\n",
    "        p = {\"input_ids\":product_input_ids.squeeze(1), \"attention_mask\":product_attention_mask.squeeze(1),\n",
    "            \"token_type_ids\":product_token_type_ids.squeeze(1)}\n",
    "        \n",
    "        q_emb_all = self.embedding_model(**q)\n",
    "        p_emb_all = self.embedding_model(**p)\n",
    "        \n",
    "        q_emb = self.mean_pooling(q_emb_all, q[\"attention_mask\"])\n",
    "        p_emb = self.mean_pooling(p_emb_all, p[\"attention_mask\"])\n",
    "        #q_emb, p_emb = self.mean_pooling()), self.model(**p)\n",
    "        \n",
    "        \n",
    "        if self.normalize:\n",
    "            q_emb = torch.nn.functional.normalize(q_emb, p=2, dim=1)\n",
    "            p_emb = torch.nn.functional.normalize(p_emb, p=2, dim=1)\n",
    "        \n",
    "        contig_labels=labels.reshape(-1).contiguous()\n",
    "        return q_emb, p_emb, contig_labels\n",
    "        \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    \n",
    "def tokenize_examples_and_target(examples):\n",
    "    queries = examples[\"query\"]\n",
    "    result = finetuned.tokenizer(queries, padding=\"max_length\", max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "    #result = finetuned.tokenizer(queries, return_tensors='pt')\n",
    "    result = {f\"query_{k}\": v for k, v in result.items()}\n",
    "\n",
    "    products = examples[\"product_title\"]\n",
    "    result_products = finetuned.tokenizer(products, padding=\"max_length\", max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "    #result_products = finetuned.tokenizer(products, return_tensors = 'pt')\n",
    "    for k, v in result_products.items():\n",
    "        result[f\"product_{k}\"] = v\n",
    "        \n",
    "    result[\"labels\"] = torch.ByteTensor([examples[\"relevance_label\"]])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "emb_loss=nn.CosineEmbeddingLoss(reduction=\"mean\")\n",
    "\n",
    "class ESCITrainer(Trainer):  \n",
    "    def compute_loss(self, model, inputs, return_outputs = False):\n",
    "        outputs = model(**inputs)\n",
    "        q_emb, p_emb, labels = outputs\n",
    "        custom_loss =  emb_loss(q_emb, p_emb, 2*labels - 1)\n",
    "        return (custom_loss, outputs) if return_outputs else custom_loss\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--pretrained_model')\n",
    "    parser.add_argument('--batch_size', type=int)\n",
    "    parser.add_argument('--eval_batch_size', type=int)\n",
    "\n",
    "    parser.add_argument('--epochs', type=int)\n",
    "    parser.add_argument('--lora_rank', default=8, type=int)\n",
    "    parser.add_argument('--lr', default=1e-6, type=float)\n",
    "\n",
    "    \n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    \n",
    "        \n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    default_args = {\"output_dir\": \"tmp\",\n",
    "                    \"evaluation_strategy\": \"steps\",\n",
    "                    \"num_train_epochs\": args.epochs,\n",
    "                    \"log_level\": \"error\",\n",
    "                    \"report_to\": \"none\"}\n",
    "\n",
    "    \n",
    "    finetuned = EncoderForESCI(args.pretrained_model, lora_rank=args.lora_rank)\n",
    "    \n",
    "    ds= load_dataset(\"smangrul/amazon_esci\")\n",
    "    processed_ds = ds.map(tokenize_examples_and_target,\n",
    "                      num_proc=os.cpu_count(), \n",
    "                     remove_columns=ds['train'].column_names)\n",
    "    \n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        gradient_accumulation_steps=4,\n",
    "        fp16=True,\n",
    "        remove_unused_columns=False, \n",
    "    **default_args,\n",
    "    )\n",
    "\n",
    "    trainer = ESCITrainer(model=finetuned, args=training_args, train_dataset=processed_ds['train'].with_format(\"torch\")\n",
    "                      , eval_dataset=processed_ds['validation'].with_format(\"torch\"))\n",
    "    trainer.train()\n",
    "\n",
    "        \n",
    "    ## Merge the models with trained Adapters, and save it to /opt/ml/code\n",
    "    merged = finetuned.embedding_model.merge_and_unload()\n",
    "    merged.save_pretrained(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bdee7f-e234-4f6a-9f75-4bf85e05ff6e",
   "metadata": {},
   "source": [
    "As you can see, it's not so different from your classical PyTorch script. Of course, hyperparameters can be upgraded: I didn't put any learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d77ff-f43e-4f2c-a7f4-cf11ee58e289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile peftscripts/requirements.txt\n",
    "\n",
    "peft\n",
    "accelerate==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafeec4-1502-46d0-a70f-4be93ee7738c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'pretrained_model': \"BAAI/bge-base-en\",\n",
    "                 'batch_size': 120,\n",
    "                 'eval_batch_size': 160,\n",
    "                 'lora_rank':8, \n",
    "                 'epochs':1, \n",
    "                 'lr':1e-6\n",
    "                 }\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./peftscripts',\n",
    "                            instance_type='ml.g4dn.xlarge',\n",
    "                            instance_count=1,\n",
    "                            role = sagemaker.get_execution_role(),\n",
    "                            transformers_version='4.26',\n",
    "                            pytorch_version='1.13',\n",
    "                            py_version='py39',\n",
    "                            hyperparameters = hyperparameters, \n",
    "                             metric_definitions=[\n",
    "                                 {'Name': 'training_loss', 'Regex': 'training loss ([0-9\\\\.]+)'},\n",
    "                                 {'Name': 'eval_loss', 'Regex': 'eval loss ([0-9\\\\.]+)'}\n",
    "                             ]\n",
    ")\n",
    "\n",
    "timing = str(int(time.time()))\n",
    "huggingface_estimator.fit(job_name=f\"PeftFTBGE{timing}\", wait=False)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
